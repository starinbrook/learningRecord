
* 架构设计的关键思维是判断和取舍，程序设计的关键思维是逻辑和实现。

========================================================================================================================================================
                       第一部分：架构设计基础：架构设计起源、架构设计的目的、常见架构复杂度分析、架构设计原则、架构设计流程
========================================================================================================================================================

1、架构是什么？

   架构是顶层设计；
   框架是面向编程或配置的半成品；
   组件是从技术维度上的复用；
   模块是从业务维度上职责的划分；
   系统是相互协同可运行的实体。
   
   框架关注的是“规范”，架构关注的是“结构”。
   框架的英文是 Framework，架构的英文是 Architecture。
   
   软件架构指软件系统的顶层结构：
   首先，“系统是一群关联个体组成”，这些“个体”可以是“子系统”“模块”“组件”等；架构需要明确系统包含哪些“个体”。
   其次，系统中的个体需要“根据某种规则”运作，架构需要明确个体运作和协作的规则。
   第三，维基百科定义的架构用到了“基础结构”这个说法，我改为“顶层结构”，可以更好地区分系统和子系统，避免将系统架构和子系统架构混淆在一起导致架构层次混乱。
   
2、架构设计历史背景

   软件架构的出现有其历史必然性。
   20 世纪 60 年代第一次软件危机引出了“结构化编程”，创造了“模块”概念；
   20 世纪 80 年代第二次软件危机引出了“面向对象编程”，创造了“对象”概念；
   到了 20 世纪 90 年代“软件架构”开始流行，创造了“组件”概念。
   我们可以看到，“模块”“对象”“组件”本质上都是对达到一定规模的软件进行拆分，差别只是在于随着软件的复杂度不断增加，拆分的粒度越来越粗，拆分的层次越来越高。
   
   一个成功的软件设计是要适应并满足业务需求，同时不断“演化”的。设计需要根据业务的变化、技术的发展不断进行“演进”，这就决定了这是一个动态活动，出现新问题，解决新问题，没有所谓的“一招鲜”。
   
   整个软件技术发展的历史，其实就是一部与“复杂度”斗争的历史，架构的出现也不例外
   
3、架构设计的目的
   
   架构设计的主要目的是为了解决软件系统复杂度带来的问题。
   
   通过熟悉和理解需求，识别系统复杂性所在的地方，然后针对这些复杂点进行架构设计。
   架构设计并不是要面面俱到，不需要每个架构都具备高性能、高可用、高扩展等特点，而是要识别出复杂点然后有针对性地解决问题。
   理解每个架构方案背后所需要解决的复杂点，然后才能对比自己的业务复杂点，参考复杂点相似的方案。

4 复杂度的来源――高性能

  （1）单台计算机内部为了高性能带来的复杂度
   手工操作―>批处理操作系统―>多进程（分时系统）―>多线程（分时系统）―>多CPU（真正并行）

   如果我们要完成一个高性能的软件系统，需要考虑如多进程、多线程、进程间通信、多线程并发等技术点，而且这些技术并不是最新的
   就是最好的，也不是非此即彼的选择。在做架构设计的时候，需要花费很大的精力来结合业务进行分析、判断、选择、组合，这个过程
   同样很复杂。  

  （2）多台计算机集群为了高性能带来的复杂度

   方式一：任务分配
          增加服务器，每个服务器上运行着相同的业务逻辑

          需要增加一个任务分配器
          任务分配器和真正的业务服务器之间有连接和交互
          任务分配器需要增加分配算法

   方式二：任务分解
          把一个大且复杂的业务系统，拆分成多个小而简单的业务系统，例如微信后台架构从逻辑上将业务进行了拆分，拆分成多个子业务，
          包括：接入/注册登陆/消息/摇一摇/漂流瓶/聊天/视频/朋友圈等，各个子业务系统单独部署到不同的服务器上，也就是说任务分解
          也增加了服务器数量，但是每个服务器上运行不同的业务逻辑
		  
5、复杂度的来源――高可用
         
    高可用：系统无中断地执行其功能的能力，代表系统的可用性程度，是进行系统设计时的准则之一。
	关键点：无中断
	系统高可用方案有很多，但本质上都是通过“冗余”来实现高可用。
    
	高可用的“冗余”解决方案，单纯从形式上来看，和高性能是一样的，都是通过增加更多机器来达到目的，但其实本质上是有根本区别的：
	高性能增加机器目的在于“扩展”处理性能；高可用增加机器目的在于“冗余”处理单元；
	
   （1）计算高可用
        
		任务分配
		        双机架构
				        主备：单纯的一台机器在跑流量，另一台机器实时地同步数据库，一旦主机宕机，备机立刻进入工作状态
						    冷备  冷备份服务器（cold server），平常情况下是关机状态，只有当主服务器宕机才开机
							温备  温备份服务器（warm server），周期性开机，根据主服务器内容进行更新，然后关机
							热备  热备份服务器（hot server），时刻处于开机状态，同主机保持同步，主机宕机时，可以随时启用热备份服务器
						主主：两台机都是同时工作，有真正做到负载均衡的作用
				集群架构（总数m + n）
				        m 主 n 备
   
   （2）存储高可用
   
        数据 + 逻辑 = 业务
		
		无论是正常情况下的传输延迟，还是异常情况下的传输中断，都会导致系统的数据在某个时间点或者时间段是不一致的，而数据的不一致又会导致业务问题；
		但如果完全不做冗余，系统的整体高可用又无法保证，所以存储高可用的难点不在于如何备份数据，而在于如何减少或者规避数据不一致对业务造成的影响。
		
	
	状态决策（高可用的基础）
	
	（1）独裁式
	     决策者：独立的决策主体，负责收集信息然后进行决策
		 上报者：所有其他冗余的个体，负责将状态信息发送给决策者
		 优点：因为只有一个决策者，所以不会出现决策混乱的问题
		 缺点：决策者本身故障时，整个系统就无法实现准确的状态决策
	
	（2）协商式
	     两个独立个体通过交流信息，然后根据规则进行决策，最常用的协商式决策就是主备决策。
		 这个架构的基本协商规则可以设计成：
         * 2 台服务器启动时都是备机。
         * 2 台服务器建立连接。
         * 2 台服务器交换状态信息。
         * 某 1 台服务器做出决策，成为主机；另一台服务器继续保持备机身份
		 优点：架构简单、规则也不复杂
		 缺点：主备连接中断时会出现决策错误
	
	（3）民主式
	     多个独立的个体通过投票的方式进行状态决策，多数取胜
		 缺点：可能会出现“脑裂”
		 
	无论采取什么样的方案，状态决策都不可能做到任何场景下都没有问题，但完全不做高可用方案又会产生更大的问题，如何选取适合系统的高可用方案，
	也是一个复杂的分析、判断和选择的过程。
	

6、复杂度的来源――可扩展性

    可扩展性 指系统为了应对将来需求变化而提供的一种扩展能力，当有新的需求出现时，系统不需要或者仅需要少量修改就可以支持，无须整个系统重构或者重建。

	设计具备良好可扩展性的系统，有两个基本条件：正确预测变化、完美封装变化。
	
	预测变化的复杂性在于：
    （1）不能每个设计点都考虑可扩展性。
    （2）不能完全不考虑可扩展性。
    （3）所有的预测都存在出错的可能性。
	
	对于架构师来说，如何把握预测的程度和提升预测结果的准确性，是一件很复杂的事情，而且没有通用的标准可以简单套上去，更多是靠自己的经验、直觉。
	
	应对变化的方案：
	方案一：将“变化”封装在一个“变化层”，将不变的部分封装在一个独立的“稳定层”。
	方案二：提炼出一个“抽象层”和一个“实现层”。抽象层是稳定的，实现层可以根据具体业务需要定制开发，当加入新的功能时，只需要增加新的实现，无须修改抽象层。
	
	
7 复杂度的来源――低成本/安全/规模

    *低成本
	
	当我们设计“高性能”“高可用”的架构时，通用的手段都是增加更多服务器来满足“高性能”和“高可用”的要求；
	而低成本正好与此相反，我们需要减少服务器的数量才能达成低成本的目标。
	因此，低成本本质上是与高性能和高可用冲突的，所以低成本很多时候不会是架构设计的首要目标，而是架构设计的附加约束。
	也就是说，我们首先设定一个成本目标，当我们根据高性能、高可用的要求设计出方案时，评估一下方案是否能满足成本目标，
	如果不行，就需要重新设计架构；如果无论如何都无法设计出满足成本要求的方案，那就只能找老板调整成本目标了。
	
	低成本给架构设计带来的主要复杂度体现在，往往只有“创新”才能达到低成本目标。
	这里的“创新”既包括开创一个全新的技术领域（这个要求对绝大部分公司太高），也包括引入新技术，
	如果没有找到能够解决自己问题的新技术，那么就真的需要自己创造新技术了。
	
	*安全
	
	从技术的角度来讲，安全可以分为两类：一类是功能上的安全，一类是架构上的安全。
	
	（1）功能安全
	常见的 XSS 攻击、CSRF 攻击、SQL 注入、Windows 漏洞、密码破解等
	本质上是因为系统实现有漏洞，黑客有了可乘之机。黑客会利用各种漏洞潜入系统，这种行为就像小偷一样，
	黑客和小偷的手法都是利用系统或家中不完善的地方潜入，并进行破坏或者盗取。因此形象地说，功能安全其实就是“防小偷”。
	
	从实现的角度来看，功能安全更多地是和具体的编码相关，与架构关系不大。
	
	很多开发框架都内嵌了常见的安全功能，能够大大减少安全相关功能的重复开发，但框架只能预防常见的安全漏洞和风险（常见的 XSS 攻击、CSRF 攻击、SQL 注入等），
	无法预知新的安全问题，而且框架本身很多时候也存在漏洞
	
	所以功能安全是一个逐步完善的过程，而且往往都是在问题出现后才能有针对性的提出解决方案，我们永远无法预测系统下一个漏洞在哪里，
	也不敢说自己的系统肯定没有任何问题。
	
	换句话讲，功能安全其实也是一个“攻”与“防”的矛盾，只能在这种攻防大战中逐步完善，不可能在系统架构设计的时候一劳永逸地解决。
	
	（2）架构安全
	如果说功能安全是“防小偷”，那么架构安全就是“防强盗”。强盗会直接用大锤将门砸开，或者用炸药将围墙炸倒；
	小偷是偷东西，而强盗很多时候就是故意搞破坏，对系统的影响也大得多。
	
	传统的架构安全主要依靠防火墙
	
	防火墙最基本的功能就是隔离网络，通过将网络划分成不同的区域，制定出不同区域之间的访问控制策略来控制不同信任程度区域间传送的数据流。
	
	防火墙的功能虽然强大，但性能一般，所以在传统的银行和企业应用领域应用较多。
	但在互联网领域，防火墙的应用场景并不多。因为互联网的业务具有海量用户访问和高并发的特点，防火墙的性能不足以支撑
	
	基于上述原因，互联网系统的架构安全目前并没有太好的设计手段来实现，更多地是依靠运营商或者云服务商强大的带宽和流量清洗的能力，较少自己来设计和实现。
	
	*规模
	
	规模带来复杂度的主要原因就是“量变引起质变”，当数量超过一定的阈值后，复杂度会发生质的变化。常见的规模带来的复杂度有：

   （1） 功能越来越多，导致系统复杂度指数级上升
       
	     系统的复杂度 = 功能数量 + 功能之间的连接数量
		 
		 例如，某个系统开始只有 3 大功能，后来不断增加到 8 大功能，虽然还是同一个系统，但复杂度已经相差很大了，具体相差多大呢？
		 假设系统间的功能都是两两相关：
		 3 个功能的系统复杂度 = 3 + 3 = 6
         8 个功能的系统复杂度 = 8 + 28 = 36
		 
   （2） 数据越来越多，系统复杂度发生质变
       
	     与功能类似，系统数据越来越多时，也会由量变带来质变，最近几年火热的“大数据”就是在这种背景下诞生的。
		 
		 目前的大数据理论基础是 Google 发表的三篇大数据相关论文，其中 
		 Google File System 是大数据文件存储的技术理论，
		 Google Bigtable 是列式数据存储的技术理论，
		 Google MapReduce 是大数据运算的技术理论，这三篇技术论文各自开创了一个新的技术领域。
		 
		 即使我们的数据没有达到大数据规模，数据的增长也可能给系统带来复杂性。
		 最典型的例子莫过于使用关系数据库存储数据，以 MySQL 为例，MySQL 单表的数据因不同的业务和应用场景会有不同的最优值，
		 但不管怎样都肯定是有一定的限度的，一般推荐在 5000 万行左右。如果因为业务的发展，单表数据达到了 10 亿行，就会产生很多问题
		 
		 因此，当 MySQL 单表数据量太大时，我们必须考虑将单表拆分为多表，这个拆分过程也会引入更多复杂性，例如：
		 拆表的规则是什么？
		 拆完表后查询如何处理？
		 
	
8、架构设计三原则

    *合适原则
	
	合适原则宣言：“合适优于业界领先”。
	真正优秀的架构都是在企业当前人力、条件、业务等各种约束下设计出来的，能够合理地将资源整合在一起并发挥出最大功效，并且能够快速落地。
	
	
	*简单原则
	
	简单原则宣言：“简单优于复杂”。
	“复杂”在制造领域代表先进，在建筑领域代表领先，但在软件领域，却恰恰相反，代表的是“问题”。
	
	软件领域的复杂性体现在两个方面：
	(1) 结构的复杂性
	结构复杂的系统几乎毫无例外具备两个特点：组成复杂系统的组件数量更多；同时这些组件之间的关系也更加复杂。
	
	结构上的复杂性存在的第一个问题是，组件越多，就越有可能其中某个组件出现故障，从而导致系统故障。
	    这个概率可以算出来，假设组件的故障率是 10%（有 10% 的时间不可用），那么
		有 3 个组件的系统可用性是（1-10%）×（1-10%）×（1-10%）= 72.9%，
		有 5 个组件的系统可用性是（1-10%）×（1-10%）×（1-10%）×（1-10%）×（1-10%）=59%，
		两者的可用性相差 13%。
	结构上的复杂性存在的第二个问题是，某个组件改动，会影响关联的所有组件，这些被影响的组件同样会继续递归影响更多的组件。
	    这个问题会影响整个系统的开发效率，因为一旦变更涉及外部系统，需要协调各方统一进行方案评估、资源协调、上线配合。
	结构上的复杂性存在的第三个问题是，定位一个复杂系统中的问题总是比简单系统更加困难。
	    首先是组件多，每个组件都有嫌疑，因此要逐一排查；
		其次组件间的关系复杂，有可能表现故障的组件并不是真正问题的根源。
		
	(2) 逻辑的复杂性
	逻辑复杂的组件，一个典型特征就是单个组件承担了太多的功能。
	
	结构的复杂性 和 逻辑的复杂性 需要达到一个平衡：
	为了降低结构复杂性，减少组件数量，那么单个组件的逻辑复杂性增高；为了降低逻辑复杂性，把复杂逻辑拆分到多个组件中，又增加了结构复杂性
	
	无论是结构的复杂性，还是逻辑的复杂性，都会存在各种问题，所以架构设计时如果简单的方案和复杂的方案都可以满足需求，最好选择简单的方案。
	
	《UNIX 编程艺术》总结的 KISS（Keep It Simple, Stupid!）原则一样适应于架构设计。
	
	
	*演化原则
	
	演化原则宣言：“演化优于一步到位”。
	
	软件架构需要根据业务发展不断变化。
	
	软件架构设计过程：
	首先，设计出来的架构要满足当时的业务需要。
    其次，架构要不断地在实际应用过程中迭代，保留优秀的设计，修复有缺陷的设计，改正错误的设计，去掉无用的设计，使得架构逐渐完善。
    第三，当业务发生变化时，架构要扩展、重构，甚至重写；代码也许会重写，但有价值的经验、教训、逻辑、设计等（类似生物体内的基因）却可以在新架构中延续。
	
	架构师在进行架构设计时需要牢记这个原则，时刻提醒自己不要贪大求全，或者盲目照搬大公司的做法。
	应该认真分析当前业务的特点，明确业务面临的主要问题，设计合理的架构，快速落地以满足业务需要，然后在运行过程中不断完善架构，不断随着业务演化架构。
	
	
10、架构设计流程第 1 步：识别复杂度

    架构设计的本质目的是为了解决软件系统的复杂性，所以在我们设计架构时，首先就要分析系统的复杂性。
	只有正确分析出了系统的复杂性，后续的架构设计方案才不会偏离方向；
	否则，如果对系统的复杂性判断错误，即使后续的架构设计方案再完美再先进，都是南辕北辙，做的越好，错的越多、越离谱。
	
	架构的复杂度主要来源于“高性能”“高可用”“可扩展”等几个方面，但架构师在具体判断复杂性的时候，不能生搬硬套，认为任何时候架构都必须同时满足这三方面的要求。
	实际上大部分场景下，复杂度只是其中的某一个，少数情况下包含其中两个，如果真的出现同时需要解决三个或者三个以上的复杂度，要么说明这个系统之前设计的有问题，
	要么可能就是架构师的判断出现了失误，即使真的认为要同时满足这三方面的要求，也必须要进行优先级排序。
	
	识别复杂度对架构师来说是一项挑战，因为原始的需求中并没有哪个地方会明确地说明复杂度在哪里，需要架构师在理解需求的基础上进行分析。
	有经验的架构师可能一看需求就知道复杂度大概在哪里；如果经验不足，那只能采取“排查法”，从不同的角度逐一进行分析。
	

11、架构设计流程第 2 步：设计备选方案

    经过架构设计第一步识别出复杂度后，就可以开始针对识别出的复杂度，设计备选方案。
	
    成熟的架构师需要对已经存在的技术非常熟悉，对已经经过验证的架构模式烂熟于心，然后根据自己对业务的理解，挑选合适的架构模式进行组合，
	再对组合后的方案进行修改和调整。
	
	虽然软件技术经过几十年的发展，新技术层出不穷，但是经过时间考验，已经被各种场景验证过的成熟技术其实更多。
	例如，高可用的主备方案、集群方案，高性能的负载均衡、多路复用，可扩展的分层、插件化等技术，
	绝大部分时候我们有了明确的目标后，按图索骥就能够找到可选的解决方案。

    只有当这种方式完全无法满足需求的时候，才会考虑进行方案的创新，而事实上方案的创新绝大部分情况下也都是基于已有的成熟技术。
	
	虽说基于已有的技术或者架构模式进行组合，然后调整，大部分情况下就能够得到我们需要的方案，但并不意味着架构设计是一件很简单的事情。
	因为可选的模式有很多，组合的方案更多，往往一个问题的解决方案有很多个；如果再在组合的方案上进行一些创新，解决方案会更多。
	因此，如何设计最终的方案，并不是一件容易的事情，这个阶段也是很多架构师容易犯错的地方。
	
	第一种常见的错误：设计最优秀的方案。
	    根据架构设计原则中“合适原则”和“简单原则“的要求，挑选合适自己业务、团队、技术能力的方案才是好方案
	第二种常见的错误：只做一个方案。
	    备选方案的数量以 3 ~ 5 个为最佳。
		备选方案的差异要比较明显。
		备选方案的技术不要只局限于已经熟悉的技术。
	第三种常见的错误：备选方案过于详细。
	    选阶段关注的是技术选型，而不是技术细节，技术选型的差异要比较明显。
		

12、架构设计流程的第 3 步：评估和选择备选方案

    步骤一：360 度环评
	    
		列出我们需要关注的质量属性点，然后分别从这些质量属性的维度去评估每个方案，再综合挑选适合当时情况的最优方案。
	
	    常见的方案质量属性点有：性能、可用性、硬件成本、项目投入、复杂度、安全性、可扩展性等。
	
	    在评估这些质量属性时，需要遵循架构设计原则 1“合适原则”和原则 2“简单原则”，避免贪大求全，基本上某个质量属性能够满足一定时期内业务发展就可以了。
	
	    遵循架构设计原则 3“演化原则”，避免过度设计、一步到位的想法。
	
	    通常情况下，如果某个质量属性评估和业务发展有关系（例如，性能、硬件成本等），需要评估未来业务发展的规模时，一种简单的方式是将当前的业务规模乘以 2 ~4 即可，
	    如果现在的基数较低，可以乘以 4；如果现在基数较高，可以乘以 2。
	
	步骤二：根据360 度环评结果选择备选方案
	
	    具体做法是按优先级选择，即架构师综合当前的业务发展情况、团队人员规模和技能、业务发展预测等因素，
	将质量属性按照优先级排序，首先挑选满足第一优先级的，如果方案都满足，那就再看第二优先级……以此类推。
	

13、架构设计流程第 4 步：详细方案设计

    简单来说，详细方案设计就是将方案涉及的 关键技术细节 给确定下来。
	
	*假如我们确定使用 Elasticsearch 来做全文搜索，那么就需要确定 Elasticsearch 的索引是按照业务划分，还是一个大索引就可以了；
	副本数量是 2 个、3 个还是 4 个，集群节点数量是 3 个还是 6 个等。

    *假如我们确定使用 MySQL 分库分表，那么就需要确定哪些表要分库分表，按照什么维度来分库分表，分库分表后联合查询怎么处理等。

    *假如我们确定引入 Nginx 来做负载均衡，那么 Nginx 的主备怎么做，Nginx 的负载均衡策略用哪个（权重分配？轮询？ip_hash？）等。
	
	答案：只需要简单根据这些技术的【适用场景】选择就可以了。
	

	
========================================================================================================================================================
                                                             第二部分：业界成熟的架构模式
========================================================================================================================================================

14、高性能数据库集群-方式一“读写分离”            分散数据库读写操作的压力
    
	读写分离原理
	读写分离的基本原理是将数据库读写操作分散到不同的节点上
	
	读写分离的基本实现
	（1）数据库服务器搭建主从集群，一主一从、一主多从都可以。
    （2）数据库主机负责读写操作，从机只负责读操作。
    （3）数据库主机通过复制将数据同步到从机，每台数据库服务器都存储了所有的业务数据。
    （4）业务服务器将写操作发给数据库主机，将读操作发给数据库从机。
	
	注意：
	这里用的是“主从集群”，而不是“主备集群”。“从机”的“从”可以理解为“仆从”，仆从是要帮主人干活的，“从机”是需要提供读数据的功能的；
	而“备机”一般被认为仅仅提供备份功能，不提供访问功能。所以使用“主从”还是“主备”，是要看场景的，这两个词并不是完全等同的。
	
	
	“读写分离”可能会引入的两个设计复杂度：主从复制延迟和分配机制
	
	复制延迟
	延迟时间内主从数据不一致，某些业务可能会出现问题，例如注册登录，用户注册（写主库）完立即登录（读从库），但是登录时提示“您还没有注册”
	
	解决主从复制延迟有几种常见的方法：
	（1）写操作后的读操作指定发给数据库主服务器
	     这种方式和业务强绑定，对业务的侵入和影响较大
	（2）读从机失败后再读一次主机
	     这就是通常所说的“二次读取”，二次读取和业务无绑定，只需要对底层数据库访问的 API 进行封装即可，实现代价较小，
		 不足之处在于如果有很多二次读取，将大大增加主机的读操作压力。
	（3）关键业务读写操作全部指向主机，非关键业务采用读写分离
	
    
	分配机制
	将读写操作区分开来，然后访问不同的数据库服务器，一般有两种方式：程序代码封装和中间件封装。
	
	程序代码封装：
	程序代码封装指在代码中抽象一个数据访问层（所以有的文章也称这种方式为“中间层封装”），实现读写操作分离和数据库服务器连接的管理。
	
	程序代码封装特点：
	（1）实现简单，而且可以根据业务做较多定制化的功能。
    （2）每个编程语言都需要自己实现一次，无法通用，如果一个业务包含多个编程语言写的多个子系统，则重复开发的工作量比较大。
    （3）故障情况下，如果主从发生切换，则可能需要所有系统都修改配置并重启。
	
	程序代码封装的开源实现方案：
	淘宝的 TDDL（Taobao Distributed Data Layer，外号: 头都大了）是比较有名的。
	它是一个通用数据访问层，所有功能封装在 jar 包中提供给业务代码调用。
	其基本原理是一个基于集中式配置的 jdbc datasource 实现，具有主备、读写分离、动态数据库配置等功能。
	
	中间件封装：
	中间件封装指的是独立一套系统出来，实现读写操作分离和数据库服务器连接的管理。
	中间件对业务服务器提供 SQL 兼容的协议，业务服务器无须自己进行读写分离。
	对于业务服务器来说，访问中间件和访问数据库没有区别，事实上在业务服务器看来，中间件就是一个数据库服务器。
	
	中间件封装特点：
	（1）能够支持多种编程语言，因为数据库中间件对业务服务器提供的是标准 SQL 接口。
    （2）数据库中间件要支持完整的 SQL 语法和数据库服务器的协议（例如，MySQL 客户端和服务器的连接协议），
	     实现比较复杂，细节特别多，很容易出现 bug，需要较长的时间才能稳定。
    （3）数据库中间件自己不执行真正的读写操作，但所有的数据库操作请求都要经过中间件，中间件的性能要求也很高。
    （4）数据库主从切换对业务服务器无感知，数据库中间件可以探测数据库服务器的主从状态。
	     例如，向某个测试表写入一条数据，成功的就是主机，失败的就是从机。
		 
	开源数据库中间件方案：
    （1）MySQL Proxy
	     MySQL 官方先是提供了 MySQL Proxy，但 MySQL Proxy 一直没有正式 GA
    （2）MySQL Router
	     现在 MySQL 官方推荐 MySQL Router。MySQL Router 的主要功能有读写分离、故障自动切换、负载均衡、连接池等
    （3）Atlas	
	     奇虎 360 公司也开源了自己的数据库中间件 Atlas，Atlas 是基于 MySQL Proxy 实现的

	应用场景：http://www.codes51.com/itwd/4270653.html
    
	
15、高性能数据库集群-方式二分库分表              分散数据库的存储压力

    常见的分散存储的方法“分库分表”，其中包括“分库”和“分表”两大类。
	
	业务分库
	业务分库指的是按照业务模块将数据分散到不同的数据库服务器。
	
	虽然业务分库能够分散存储和访问压力，但同时也带来了新的问题
	1.join 操作问题
	2. 事务问题
	3. 成本问题
	
	分表
	单表数据拆分有两种方式：垂直分表和水平分表
	
	垂直分表
	垂直分表适合将表中某些不常用且占了大量空间的列拆分出去。
	垂直分表引入的复杂性主要体现在表操作的数量要增加。
	
	水平分表
	水平分表适合表行数特别大的表，有的公司要求单表行数超过 5000 万就必须进行分表，这个数字可以作为参考，但并不是绝对标准，关键还是要看表的访问性能。
	对于一些比较复杂的表，可能超过 1000 万就要分表了；而对于一些简单的表，即使存储数据超过 1 亿行，也可以不分表。
	但不管怎样，当看到表的数据量达到千万级别时，作为架构师就要警觉起来，因为这很可能是架构的性能瓶颈或者隐患。
	
	水平分表相比垂直分表，会引入更多的复杂性，主要表现在下面几个方面：
	*路由
	水平分表后，某条数据具体属于哪个切分后的子表，需要增加路由算法进行计算，这个算法会引入一定的复杂性。
	常见的路由算法有：
	    范围路由
	    Hash 路由
	    配置路由
    *join 操作
	*count() 操作
	*order by 操作
	
	
16 高性能NoSQL

    关系数据库强大的SQL功能和ACID属性，使得关系数据库广泛应用于各式各样的系统中，但这并不意味着关系数据库是完美的，关系数据库存在以下缺点：        
	（1）关系数据库存储的是行记录，无法存储数据结构
    （2）关系数据库的schema扩展很不方便，也就是增加修改字段不方便
    （3）关系数据库在大数据场景下I/O较高
    （4）关系数据库的全文搜索功能比较弱
   
    针对上述问题，分别诞生了不同的NoSQL解决方案，这些方案与关系数据库相比，在某些应用场景下表现更好。
    NoSQL方案带来的优势，本质上是牺牲ACID中的某个或某几个特性，因此我们不能盲目迷信NoSQL是银弹，而应该将NoSQL作为SQL的一个有力补充。
    NoSQL 不是 No SQL，而是Not only SQL

    常见的NoSQL方案分为4类：
    K-V	存储：解决关系数据库无法存储数据结构的问题，以Redis为代表
    文档数据库：解决关系数据库强schema约束的问题，以MongoDB为代表
    列式数据库：解决关系数据库大数据场景下的I/O问题，以HBase为代表
    全文搜索引擎：解决关系数据库的全文搜索性能问题，以Elasticsearch为代表
	

17、高性能缓存架构

    虽然我们可以通过各种手段来提升存储系统的性能，但在某些复杂的业务场景下，单纯依靠存储系统的性能提升不够的，典型的场景有：
   （1）需要经过复杂运算后得出的数据，存储系统无能为力
   （2）读多写少的数据，存储系统有心无力
	缓存就是为了弥补存储系统在这些复杂业务场景下的不足，其基本原理是将可能重复使用的数据放到内存中，一次生成、多次使用，避免每次使用都去访问存储系统。
	
    缓存的架构设计要点：
	
   （1）缓存穿透
	缓存穿透是指缓存没有发挥作用，业务系统虽然去缓存查询数据，但缓存中没有数据，业务系统需要再次去存储系统查询数据。
   （2）缓存雪崩
	缓存雪崩是指当缓存失效（过期）后引起系统性能急剧下降的情况。
        缓存雪崩的常见解决方法有两种：更新锁机制和后台更新机制。
        a. 更新锁
        b. 后台更新
   （3）缓存热点
	虽然缓存系统本身的性能比较高，但对于一些特别热点的数据，如果大部分甚至所有的业务请求都命中同一份缓存数据，则这份数据所在的缓存服务器的压力也很大。
        缓存热点的解决方案就是复制多份缓存副本，将请求分散到多个缓存服务器上，减轻缓存热点导致的单台缓存服务器压力。
	缓存副本设计有一个细节需要注意，就是不同的缓存副本不要设置统一的过期时间，否则就会出现所有缓存副本同时生成同时失效的情况，从而引发缓存雪崩效应。
        正确的做法是设定一个过期时间范围，不同的缓存副本的过期时间是指定范围内的随机值。
		 

18、单服务器高性能模式：PPC 与 TPC

	高性能架构设计主要集中在两方面：
	（1）尽量提升单服务器的性能，将单服务器的性能发挥到极致。
	（2）如果单服务器无法支撑性能，设计服务器集群方案。
	
	架构设计决定了系统性能的上限，实现细节决定了系统性能的下限。
	
	单服务器高性能的关键之一就是服务器采取的并发模型，并发模型有如下两个关键设计点：
	（1）服务器如何管理连接。
	（2）服务器如何处理请求。
	以上两个设计点最终都和操作系统的I/O模型及进程模型相关：
	I/O模型：阻塞、非阻塞、同步、异步
	进程模型：单进程、多进程、多线程
	
	-----------------------------------------------------------------------------------------------------------------------------------------------------------
	分得清阻塞和非阻塞 以及 同步与异步，但就是搞不懂 阻塞和同步有啥不同，刚好知乎上有一个回答解开了我的困惑：
	作者：严肃
    链接：https://www.zhihu.com/question/19732473/answer/20851256
    来源：知乎
    著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

    1.同步与异步同步和异步关注的是消息通信机制 (synchronous communication/ asynchronous communication)
    所谓同步，就是在发出一个*调用*时，在没有得到结果之前，该*调用*就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由*调用者*主动等待这个*调用*的结果。
    而异步则是相反，*调用*在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。
	而是在*调用*发出后，*被调用者*通过状态、通知来通知调用者，或通过回调函数处理这个调用。
	典型的异步编程模型比如Node.js 
	举个通俗的例子：
	你打电话问书店老板有没有《分布式系统》这本书，如果是同步通信机制，书店老板会说，你稍等，”我查一下"，然后开始查啊查，
	等查好了（可能是5秒，也可能是一天）告诉你结果（返回结果）。
	而异步通信机制，书店老板直接告诉你我查一下啊，查好了打电话给你，然后直接挂电话了（不返回结果）。
	然后查好了，他会主动打电话给你。在这里老板通过“回电”这种方式来回调。
	2. 阻塞与非阻塞阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态.
	阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。
	非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。
	还是上面的例子，
	你打电话问书店老板有没有《分布式系统》这本书，你如果是阻塞式调用，你会一直把自己“挂起”，直到得到这本书有没有的结果，
	如果是非阻塞式调用，你不管老板有没有告诉你，你自己先一边去玩了， 当然你也要偶尔过几分钟check一下老板有没有返回结果。
	在这里阻塞与非阻塞与是否同步异步无关。跟老板通过什么方式回答你结果无关。
	-----------------------------------------------------------------------------------------------------------------------------------------------------------
	
    PPC
	PPC 是 Process Per Connection 的缩写，其含义是指每次有新的连接就新建一个进程去专门处理这个连接的请求，这是传统的 UNIX 网络服务器所采用的模型。
	PPC 模式实现简单，比较适合服务器的连接数没那么多的情况，例如数据库服务器。
	PPC 方案弊端：
	  fork 代价高
	  父子进程通信复杂
	  支持的并发连接数量有限
	
	TPC
	TPC 是 Thread Per Connection 的缩写，其含义是指每次有新的连接就新建一个线程去专门处理这个连接的请求。
	TPC 方案弊端：
	  创建线程虽然比创建进程代价低，但并不是没有代价，高并发时（例如每秒上万连接）还是有性能问题。
	  无须进程间通信，但是线程间的互斥和共享又引入了复杂度，可能一不小心就导致了死锁问题。
	  多线程会出现互相影响的情况，某个线程出现异常时，可能导致整个进程退出（例如内存越界）。
	  
	对比 PPC 和 TPC，总结来说，PPC稳定性高，PPC支持并发连接数高
	
	
19 单服务器高性能架构：Reactor 和 Proactor

    单服务器高性能的PPC 和 TPC，它们的优点都是实现简单，缺点是都无法支撑高并发的场景，Reactor 和 Proactor正是为此为生的。

    Reactor
    
    引入进程池/线程池（统称为资源池），实现资源复用，这样就不需要单独为每个连接创建进程/线程，而是创建一个资源池，将连接分配给资源池中的某个
    进程/线程来处理，处理完后进程/线程重新返回资源池等待下一个连接。这样，一个进程/线程就可以处理多个连接业务。

    一个进程/线程，一个连接的情况下，进程/线程可以采用“read-->业务处理-->write”的处理流程，如果当前连接没有数据可读，则进程就阻塞在read操作上。
    但是，如果是一个进程/线程，多个连接的情况下，如果进程阻塞在某个连接的read操作上，那么此时即便其他连接上有数据可读，进程/线程也无法去处理，
    很显然这样是无法做到高性能的。为了解决这个问题，引入I/O多路复用技术（https://www.zhihu.com/question/32163005）。

    I/O多路复用结合线程池，完美地解决了PPC 和 TPC的问题，而且“大神们”给它取了一个很牛的名字：Reactor。
    Reactor模式也叫Dispatcher模式，Dispatcher更加贴近模式本身含义，即I/O多路复用统一监听事件，收到事件后分配（Dispatch）给资源池中的某个进程/线程。

    Reactor模式的核心组成部分包括Reactor和资源池（进程池或线程池），其中Reactor负责监听和分配事件,资源池负责处理事件。
    Reactor模式的具体实现方案灵活多变，主要体现在：Reactor的数量可以变化;资源池中资源的数量可以变化。
    最终Reactor模式有三种典型的实现方案：
    单Reactor单进程/线程：只适用于业务处理非常快速的场景，目前比较著名的开源软件中使用单Reactor单进程/线程的是Redis
    单Reactor多线程：
    多Reactor多进程/线程：目前著名开源系统Nginx采用的是多Reactor多进程，Memecache和Netty采用的是多Reactor多线程

    Proactor

    Reactor是非阻塞同步网络模型，因为真正的read和send操作都需要用户进程同步操作。这里的“同步”指用户进程在执行read 和 send这类I/O操作的时候是
    同步的，如果把I/O操作改为异步就能够进一步提升性能，这就是异步网络模型Proactor。
	
20、高性能负载均衡：分类及架构

    单服务器无论如何优化、无论采用多好的硬件，总会有一个性能天花板，当单台服务器的性能无法满足业务需求时，就需要设计高性能集群来提升系统整体的处理性能。
	
	高性能集群的本质：通过增加更多的服务器来提升系统整体的计算能力。由于计算本身存在一个特点：同样的输入数据和逻辑，无论在哪台服务器上执行，都应该得到相同的
	输出。因此，高性能集群设计的复杂度主要体现在任务分配上，需要设计合理的任务分配策略，将计算任务分配到多台服务器上执行。
	
	高性能集群的复杂性主要体现在需要增加一个任务分配器，以及为任务选择一个合适的任务分配算法。
	对于任务分配器，现在更流行的通用叫法是“负载均衡器”（这个名词有一定的误导性，负载均衡不只是为了计算单元的负载达到均衡状态）
	
	常见的负载均衡系统包括3类：DNS负载均衡、硬件负载均衡、软件负载均衡。
	
	DNS负载均衡：
	
	DNS是最简单也是最常见的负载均衡方式，一般用来实现地理级别的均衡。例如，北京的用户访问北京的机房，南方的用户访问深圳的机房。
	DNS负载均衡的本质是DNS解析同一个域名可以返回不同的IP地址。
	
	DNS负载均衡实现简单、成本低，但也存在粒度太粗、负载均衡算法少等缺点。
	优点：(1)简单、成本低。负载均衡工作直接交给DNS服务器处理，无需自己开发或者维护负载均衡设备。
	      (2)就近访问，提升访问速度。DNS解析时可以根据请求来源IP，解析成距离用户最近的服务器地址，可以加快访问速度、改善性能。
	缺点：(1)更新不及时。DNS缓存时间比较长，修改DNS配置后，由于缓存的原因，还是有很多用户会继续访问修改前的IP，这样的访问失败，达不到负载均衡的目的，并且也影响用户
	         正常使用业务。
		  (2)扩展性差。DNS负载均衡的控制权在域名商那里，无法根据业务特点针对其做更多的定制化功能和扩展特性。
		  (3)分配策略比较简单。DNS负载均衡支持的算法少；不能区分服务器的差异，也无法感知后端服务器的状态。
		  
	硬件负载均衡：
	
	硬件负载均衡是通过单独的硬件设备来实现负载均衡功能，这类设备和路由器、交换机类似，可以理解为一个用于负载均衡的基础网络设备。
	目前业界典型的硬件负载均衡设备有两款：F5 和 A10。（这类设备性能强劲、功能强大，但价格不菲，“土豪”公司才用得起）
	
	硬件负载均衡优缺点
	优点：(1)功能强大
	      (2)性能强大 （软件负载均衡最多支持10万级并发，硬件负载均衡可以支持100万级以上的并发）
		  (3)稳定性高
		  (4)支持安全防护
	缺点：(1)价格昂贵
	      (2)扩展能力差
		  
	软件负载均衡：
	
	软件负载均衡通过负载均衡软件来实现负载均衡，常见的有Nginx 和 LVS（Linux Virtual Sever，http://www.linuxvirtualserver.org/zh/lvs1.html），
	其中Nginx 是软件的7层负载均衡，LVS 是Linux内核的4层负载均衡。4层和7层的区别就在于协议和灵活性，Nginx支持HTTP、E-mainl协议；而LVS是4层负载均衡，和协议无关，
	几乎所有应用都可以做，例如，聊天、数据库等。
	
	软件负载均衡和硬件负载均衡，最主要的区别就在于性能，硬件负载均衡性能远远高于软件负载均衡性能。Nginx性能是万级的；LVS性能是十万级的；而F5性能是百万级。
	
	软件负载均衡优缺点：
	优点：(1)简单
	      (2)便宜
		  (3)灵活
    缺点：(1)性能一般
	      (2)功能没有硬件负载均衡那么强大
		  (3)一般不具备防火墙和防DDoS攻击等安全功能
		  
	负载均衡典型架构
	组合DNS负载均衡、硬件负载均衡和软件负载均衡；
	组合基本原则：DNS负载均衡用于实现地理级别的负载均衡（把请求任务分配到哪个地区）
	              硬件负载均衡用于实现集群级别的负载均衡（把请求任务分配到哪个集群）
				  软件负载均衡用于实现机器级别的负载均衡（把请求任务分配到哪个机器）
    
21、高性能负载均衡：算法

    负载均衡算法，根据算法目的，大体上可以分为以下几类：
	（1）任务平分类：负载均衡系统将收到的任务平均分配给服务器进行处理，这里“平均”可以是绝对数量的平均，也可以是比例或者权重上的平均
	（2）负载均衡类：负载均衡系统根据服务器的负载来进行分配，这里的负载并不一定是通常意义上我们说的“CPU负载”，而是系统当前的压力，
	                 可以用CPU负载来衡量，也可以用连接数、I/O使用率、网卡吞吐量等来衡量系统的压力
	（3）性能最优类：负载均衡系统根据服务器的响应时间来进行任务分配，优先将任务分配给响应最快的服务器
	（4）Hash类：负载均衡系统根据任务中的某些关键信息进行Hash运算，将相同Hash值的请求分配到同一台服务器上。
	             常见的有源地址Hash、目标地址Hash、session id hash、用户ID Hash等
				 
	轮询
	负载均衡系统收到请求后，按照顺序轮流分配到服务器上
	轮询是最简单的一个策略，无须关注服务器本身的状态
	“简单”是轮询算法的优点，也是它的缺点
	
	加权轮询
	负载均衡系统根据服务器权重进行任务分配，这里的权重一般是根据硬件配置进行静态配置的，采用动态的方式计算会更加契合业务，但复杂度也会更高。
	加权轮询解决了轮询算法中无法根据服务器的配置差异进行任务分配的问题，但同样存在无法根据服务器的状态差异进行任务分配的问题。
	
	负载最低优先
	负载均衡系统将任务分配给当前负载最低的服务器，这里的负载根据不同的任务类型和业务场景，可以用不同的指标来衡量。
	负载最低优先算法基本上能够比较完美地解决轮询算法的缺点，因为采用这种算法后，负载均衡系统需要感知服务器当前的运行状态。
	当然，其代价是复杂度大幅上升。所以负载最低优先算法虽然效果看起来很美好，但实际上真正应用的场景反而没有轮询（包括加权轮询）那么多。
	
	性能最优类
	负载最低优先算法是站在服务器的角度来进行分配的，而性能最优优先类算法是站在客户端的角度来进行分配的，优先将任务分配给处理速度最快的服务器，
	通过这种方式达到最快响应客户端的目的。
	和负载最低优先类算法类似，性能最优优先类算法本质上也是感知了服务器的状态，只是通过响应时间这个外部标准来衡量服务器状态而已。因此性能最优优先类算法
	存在问题和负载最低优先类算法类似，复杂度都很高。
	
	Hash类 
	负载均衡系统根据任务中的某些关键信息进行Hash运算，将相同Hash值的请求分配到同一台服务器上，这样做的目的是为了满足特定的业务需求，如：
	*源地址 Hash
	将来源于同一个源IP地址的任务分配给同一个服务器进行处理，适合于存在事务、会话的业务。
	*ID Hash
	将某个ID标识的业务分配到同一个服务器中进行处理，这里的ID一般是临时性数据的ID（如session ID）

22、CAP 理论

    CAP 定理（CAP theorem）又被称作布鲁尔定理（Brewer's theorem），
	是加州大学伯克利分校的计算机科学家埃里克・布鲁尔（Eric Brewer）在 2000 年的 ACM PODC 上提出的一个猜想。
	2002 年，麻省理工学院的赛斯・吉尔伯特（Seth Gilbert）和南希・林奇（Nancy Lynch）发表了布鲁尔猜想的证明，使之成为分布式计算领域公认的一个定理。
	
	Consistency         一致性
	Availability        可用性
	Partition Tolerance 分区容错性
	
	CAP 定理：
	In a distributed system (a collection of interconnected nodes that share data.), 
	you can only have two out of the following three guarantees across a write/read pair: 
	Consistency, Availability, and Partition Tolerance 
	- one of them must be sacrificed.
	
	Consistency：
	A read is guaranteed to return the most recent write for a given client.
	
	Availability：
	A non-failing node will return a reasonable response within a reasonable amount of time (no error or timeout).
	
	Partition Tolerance：
	The system will continue to function when network partitions occur.
	
	what's network partitions ???
        假设本来5台机器网络都是通的，现在由于交换机故障，其中3台联通形成小团体A，另外两台联通形成小团体B，但是A和B不联通
        
        CAP 应用
        虽然 CAP 理论定义是三个要素中只能取两个，但放到分布式环境下来思考，我们会发现必须选择 P（分区容忍）要素，因为网络本身无法做到 100% 可靠，
        有可能出故障，所以分区是一个必然的现象。
        如果我们选择了 CA 而放弃了 P，那么当发生分区现象时，为了保证 C，系统需要禁止写入，当有写入请求时，系统返回 error（例如，当前系统不允许写入）
        ，这又和 A 冲突了，因为 A 要求返回 no error 和 no timeout。
        因此，分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。
        

23、CAP 细节

    CAP 关键细节点：
    （1）CAP关注的粒度是数据，而不是整个系统。
    （2）CAP是忽略网络延迟的。
    （3）正常运行情况下（不发生网络分区），不存在CP和AP的选择，可以同时满足CA。
    （4）放弃并不等于什么都不做，需要为分区恢复后做准备。

    ACID
    ACID 是数据库管理系统为了保证事务的正确性而提出来的一个理论，ACID 包含四个约束：
    （1）Atomicity（原子性）
     一个事务中的所有操作，要么全部完成，要么全部不完成，不会在中间某个环节结束。
     事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。
    （2）Consistency（一致性）
     在事务开始之前和事务结束以后，数据库的完整性没有被破坏。
    （3）Isolation（隔离性）
     数据库允许多个并发事务同时对数据进行读写和修改的能力。
     隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。
    （4）Durability（持久性）
     事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

    BASE
    BASE 是指基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventual Consistency），
    核心思想是即使无法做到强一致性（CAP 的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性。
    （1）基本可用（Basically Availability）
     分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。
    （2）软状态（Soft State）
     允许系统存在中间状态，而该中间状态不会影响系统整体可用性。这里的中间状态就是 CAP 理论中的数据不一致。
    （3）最终一致性（Enventual Consistency）
     系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。

    综上，ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。

24、FMEA方法，排除架构可用性隐患的利器

    FMEA 介绍
	FMEA（Failure mode and effects analysis，故障模式与影响分析）。FMEA 是一种在各行各业都有广泛应用的可用性分析方法，通过对系统范围内潜在的故障模式加以分析，
	并按照严重程度进行分类，以确定失效对于系统的最终影响。
	FMEA 最早是在美国军方开始应用的，但 FMEA 方法现在已广泛应用于各种各样的行业，包括半导体加工、餐饮服务、塑料制造、软件及医疗保健行业。
	FMEA 之所以能够在这些差异很大的领域都得到应用，根本原因在于 FMEA 是一套分析和思考的方法，而不是某个领域的技能或者工具。
	
	FMEA 方法
	在架构设计领域，FMEA 的具体分析步骤是：
    (1)给出初始的架构设计图。
    (2)假设架构中某个部件发生故障。
    (3)分析此故障对系统功能造成的影响。
    (4)根据分析结果，判断架构是否需要进行优化。
	
	FMEA 分析工具： FMEA 分析表
	常见的 FMEA 分析表格包含下面部分
	---------------------------------------------------------------------------------------------------------------------------------------------------
	|    功能点  |   故障模式  |  故障影响  |  严重程度  |  故障原因  |  故障概率  |  风险程度  |  已有措施  |  规避措施  |  解决措施  |  后续规划    |
	---------------------------------------------------------------------------------------------------------------------------------------------------
	
	说明
	(1) 功能点：从用户角度来看的，而不是从系统各个模块功能点划分来看的。
	(2) 故障模式：故障模式指的是系统会出现什么样的故障，包括故障点和故障形式。
	              只需要假设出现某种故障现象即可，故障模式的描述要尽量精确，多使用量化描述，避免使用泛化的描述。
				  例如，推荐使用“MySQL 响应时间达到 3 秒”，而不是“MySQL 响应慢”。
	(3) 故障影响：当发生故障模式中描述的故障时，功能点具体会受到什么影响。
	              常见的影响有：功能点偶尔不可用、功能点完全不可用、部分用户功能点不可用、功能点响应缓慢、功能点出错等。
				  故障影响也需要尽量准确描述。例如，推荐使用“20% 的用户无法登录”，而不是“大部分用户无法登录”。
	(4) 严重程度：严重程度指站在业务的角度故障的影响程度，一般分为“致命 / 高 / 中 / 低 / 无”五个档次。
	              严重程度按照这个公式进行评估：严重程度 = 功能点重要程度 × 故障影响范围 × 功能点受损程度。
	(5) 故障原因：“故障模式”中只描述了故障的现象，并没有单独列出故障原因。主要原因在于不管什么故障原因，故障现象相同，对功能点的影响就相同。
	              这里单独列出故障原因的理由是：不同的故障原因发生概率不相同
				                                不同的故障原因检测手段不一样
				                                不同的故障原因的处理措施不一样
	(6) 故障概率：这里的概率就是指某个具体故障原因发生的概率。
	              一般分为“高 / 中 / 低”三档即可，具体评估的时候需要有以下几点需要重点关注。
				  硬件：硬件随着使用时间推移，故障概率会越来越高。
				  开源系统：成熟的开源系统 bug 率低，刚发布的开源系统 bug 率相比会高一些；
				            自己已经有使用经验的开源系统 bug 率会低，刚开始尝试使用的开源系统 bug 率会高。
				  自研系统：和开源系统类似，成熟的自研系统故障概率会低，而新开发的系统故障概率会高。
	(7) 风险程度：风险程度就是综合严重程度和故障概率来一起判断某个故障的最终等级，风险程度 = 严重程度 × 故障概率。
	              因此可能出现某个故障影响非常严重，但其概率很低，最终来看风险程度就低。
	(8) 已有措施：针对具体的故障原因，系统现在是否提供了某些措施来应对，包括：检测告警、容错、自恢复等。
	(9) 规避措施：规避措施指为了降低故障发生概率而做的一些事情，可以是技术手段，也可以是管理手段。
	(10)解决措施：解决措施指为了能够解决问题而做的一些事情，一般都是技术手段。
	(11)后续规划：综合前面的分析，就可以看出哪些故障我们目前还缺乏对应的措施，哪些已有措施还不够，针对这些不足的地方，再结合风险程度进行排序，给出后续的改进规划。
	              这些规划既可以是技术手段，也可以是管理手段；可以是规避措施，也可以是解决措施。同时需要考虑资源的投入情况，优先将风险程度高的系统隐患解决。
				  
25 高可用存储架构：双机架构

    存储高可用方案的本质都是通过将数据复制到多个存储设备，通过数据冗余的方式来实现高可用，其复杂性主要体现在如何应对复制延迟和中断导致的数据不一致问题。
    对任何一个高可用存储方案，需要从以下几个方面思考和分析：
    （1）数据如何复制？
    （2）各个节点的职责是什么？
    （3）如何应对复制延迟？
    （4）如何应对复制中断？

    常见的高可用存储架构有主备 主从 主主 集群 分区

    主备复制（内部的后台管理系统使用主备复制架构的情况会比较多）
    主备复制是最常见也是最简单的一种存储高可用方案
    主备架构中的“备机”主要还是起到一个备份作用，并不承担实际的业务读写操作，如果要把备机改为主机，需要人工操作。
    
    主从复制（写少读多的业务使用主从复制的存储架构比较多）
    主机负责读写操作，从机只负责读操作，不负责写操作。故障时需要人工干预。

    双机切换

    根据状态传递渠道的不同，常见的主备切换架构有三种形式：互连式 中介式 和 模拟式。
    
    主主复制（一般适合于那些临时性、可丢失、可覆盖的数据场景）
    主主复制指的是两台机器都是主机，互相将数据复制给对方，客户端可以任意挑选其中一台机器进行读写操作。
    
26、高可用存储架构：数据集群和数据分区

    数据集群
	简单来说，集群就是多台机器组合在一起形成一个统一的系统，这里的“多台”，数量上至少是3台。
	根据集群中机器承担的不同角色来划分，集群可以分为两类：数据集中集群、数据分散集群。
	
	数据分区
    数据分区指将数据按照一定的规则进行分区，不同分区分布在不同的地理位置上，每个分区存储一部分数据，通过这种方式来规避地理级别的故障所造成的巨大影响。
	采用了数据分区的架构后，即使某个地区发生严重的自然灾害或者事故，受影响的也只是一部分数据，而不是全部数据都不可用，当故障恢复后，其他地区备份的数据
	也可以帮助故障地区快速恢复业务。







